# DOL 2.0 Implementation Phase: Lexer + Parser + Tests
# Execute: claude-flow run claude-flow-dol2-impl.yaml --workflow impl-phase-1

name: dol2-impl-phase1
version: 0.1.0
description: |
    Extend DOL lexer and parser for 2.0 syntax.
    - Add composition, meta, and control flow tokens
    - Implement Pratt parser for operator precedence
    - Add control flow and lambda parsing
    - Update test suites

config:
    max_concurrent_agents: 3
    checkpoint_interval: 30m
    working_dir: ~/repos/univrs-metadol
    log_level: info

context:
    project: DOL 2.0 Implementation
    phase: "Lexer + Parser Extensions"

    existing_files:
        lexer: src/lexer.rs
        parser: src/parser.rs
        ast: src/ast.rs
        lexer_tests: tests/lexer_tests.rs
        parser_tests: tests/parser_tests.rs

    reference_specs:
        - stdlib/operator.precedence.md
        - docs/specification.md

    spec_files:
        types: "spec/types/*.spec.dol"
        control: "spec/control/*.spec.dol"
        compose: "spec/compose/*.spec.dol"
        meta: "spec/meta/*.spec.dol"

# ═══════════════════════════════════════════════════════════════════
# AGENTS
# ═══════════════════════════════════════════════════════════════════

agents:
    # ─────────────────────────────────────────────────────────────────
    # LEXER IMPLEMENTATION
    # ─────────────────────────────────────────────────────────────────
    dol-lexer-impl:
        role: implementation
        model: claude-sonnet-4-20250514
        description: Extend Logos-based lexer for DOL 2.0 tokens

        context: |
            Current lexer location: src/lexer.rs
            Framework: Logos (derive macro based lexer)

            The lexer must be extended to recognize DOL 2.0 tokens while
            maintaining backward compatibility with DOL 1.x syntax.

            Reference: stdlib/operator.precedence.md for operator symbols

        tools:
            - file_read
            - file_write
            - cargo_build
            - cargo_test

        inputs:
            - src/lexer.rs
            - stdlib/operator.precedence.md

        outputs:
            - src/lexer.rs (modified)
            - src/tokens.rs (new, if needed for organization)

        task: |
            Extend the DOL lexer (src/lexer.rs) to recognize DOL 2.0 tokens.

            ## New Tokens to Add

            ### Composition Operators
            ```rust
            #[token("|>")]
            Pipe,           // Forward pipe

            #[token(">>")]
            Compose,        // Function composition

            #[token("@")]
            Apply,          // Function application

            #[token(":=")]
            Bind,           // Monadic bind

            #[token("<|")]
            BackPipe,       // Backward pipe (optional)
            ```

            ### Meta-Programming Operators
            ```rust
            #[token("'")]
            Quote,          // AST capture

            #[token("!")]
            Eval,           // AST execution (careful: also logical not)

            #[token("#")]
            Macro,          // Macro invocation

            #[token("?")]
            Reflect,        // Type reflection

            #[token("[|")]
            IdiomOpen,      // Idiom bracket open

            #[token("|]")]
            IdiomClose,     // Idiom bracket close
            ```

            ### Control Flow Keywords
            ```rust
            #[token("if")]
            If,

            #[token("else")]
            Else,

            #[token("match")]
            Match,

            #[token("for")]
            For,

            #[token("while")]
            While,

            #[token("loop")]
            Loop,

            #[token("break")]
            Break,

            #[token("continue")]
            Continue,

            #[token("return")]
            Return,

            #[token("in")]
            In,

            #[token("where")]
            Where,
            ```

            ### Lambda and Type Syntax
            ```rust
            #[token("->")]
            Arrow,          // Return type / lambda arrow

            #[token("=>")]
            FatArrow,       // Match arm / closure

            #[token("|")]
            Bar,            // Lambda parameter delimiter / match patterns

            #[token("_")]
            Underscore,     // Wildcard pattern
            ```

            ### Type Keywords
            ```rust
            #[token("Int8")]
            Int8,
            #[token("Int16")]
            Int16,
            #[token("Int32")]
            Int32,
            #[token("Int64")]
            Int64,
            #[token("UInt8")]
            UInt8,
            #[token("UInt16")]
            UInt16,
            #[token("UInt32")]
            UInt32,
            #[token("UInt64")]
            UInt64,
            #[token("Float32")]
            Float32,
            #[token("Float64")]
            Float64,
            #[token("Bool")]
            BoolType,
            #[token("String")]
            StringType,
            #[token("Void")]
            VoidType,
            ```

            ## Disambiguation Rules

            1. `!` is both Eval and logical Not
               - As prefix before `{`: Eval (e.g., `!{ expr }`)
               - As prefix before identifier/value: Not (e.g., `!flag`)
               - Context resolved in parser, lexer emits single `Bang` token

            2. `|` is both Bar (lambda) and bitwise Or
               - `|x|` = lambda params
               - `a | b` = bitwise or
               - Lexer emits `Bar`, parser resolves by context

            3. `'` is Quote, not character literal
               - DOL uses double quotes for strings
               - Single quote is always Quote operator

            ## Validation

            After modification:
            ```bash
            cargo build
            cargo test lexer
            ```

            All existing tests must pass. New tokens must not break DOL 1.x files.

    # ─────────────────────────────────────────────────────────────────
    # PARSER IMPLEMENTATION
    # ─────────────────────────────────────────────────────────────────
    dol-parser-impl:
        role: implementation
        model: claude-sonnet-4-20250514
        description: Extend parser with Pratt parsing and control flow

        context: |
            Current parser location: src/parser.rs
            Style: Recursive descent
            AST definitions: src/ast.rs

            The parser needs:
            1. Pratt parser for expression precedence
            2. Control flow statement parsing
            3. Lambda expression parsing
            4. Pattern matching parsing

            Reference: stdlib/operator.precedence.md for precedence table

        tools:
            - file_read
            - file_write
            - cargo_build
            - cargo_test

        inputs:
            - src/parser.rs
            - src/ast.rs
            - src/lexer.rs (after dol-lexer-impl)
            - stdlib/operator.precedence.md

        outputs:
            - src/parser.rs (modified)
            - src/ast.rs (modified with new node types)
            - src/pratt.rs (new, Pratt parser module)

        dependencies:
            - dol-lexer-impl

        task: |
            Extend the DOL parser for DOL 2.0 syntax.

            ## Part 1: Pratt Parser for Expressions

            Create src/pratt.rs with precedence climbing:

            ```rust
            //! Pratt parser for DOL expression precedence

            use crate::lexer::Token;
            use crate::ast::Expr;

            /// Binding power (left, right) for operators
            /// Higher = tighter binding
            pub fn binding_power(op: &Token) -> Option<(u8, u8)> {
                match op {
                    // Member access (tightest)
                    Token::Dot => Some((90, 91)),

                    // Function composition (right-assoc)
                    Token::Compose => Some((70, 71)),

                    // Function application (right-assoc)
                    Token::Apply => Some((60, 61)),

                    // Comparison
                    Token::Eq | Token::Ne => Some((40, 41)),
                    Token::Lt | Token::Le | Token::Gt | Token::Ge => Some((40, 41)),

                    // Arithmetic
                    Token::Plus | Token::Minus => Some((50, 51)),
                    Token::Star | Token::Slash | Token::Percent => Some((55, 56)),

                    // Logical
                    Token::And => Some((30, 31)),
                    Token::Or => Some((25, 26)),

                    // Pipe (left-assoc)
                    Token::Pipe => Some((21, 20)),

                    // Bind (left-assoc, loosest)
                    Token::Bind => Some((11, 10)),

                    _ => None,
                }
            }

            /// Prefix binding power
            pub fn prefix_binding_power(op: &Token) -> Option<u8> {
                match op {
                    Token::Minus => Some((0, 80)),  // Unary minus
                    Token::Bang => Some((0, 80)),   // Logical not / Eval
                    Token::Quote => Some((0, 85)),  // Quote
                    Token::Reflect => Some((0, 85)), // Reflect
                    _ => None,
                }
            }
            ```

            Integrate into parser:

            ```rust
            fn parse_expr(&mut self, min_bp: u8) -> Result<Expr, ParseError> {
                // Parse prefix / atom
                let mut lhs = self.parse_prefix_or_atom()?;

                loop {
                    // Check for infix operator
                    let op = match self.peek() {
                        Some(tok) => tok.clone(),
                        None => break,
                    };

                    let (l_bp, r_bp) = match binding_power(&op) {
                        Some(bp) => bp,
                        None => break,
                    };

                    if l_bp < min_bp {
                        break;
                    }

                    self.advance(); // consume operator
                    let rhs = self.parse_expr(r_bp)?;

                    lhs = Expr::Binary {
                        left: Box::new(lhs),
                        op,
                        right: Box::new(rhs),
                    };
                }

                Ok(lhs)
            }
            ```

            ## Part 2: AST Node Extensions

            Add to src/ast.rs:

            ```rust
            /// Expression nodes
            #[derive(Debug, Clone)]
            pub enum Expr {
                // Existing...
                Literal(Literal),
                Identifier(String),

                // Binary operations (using Pratt parser)
                Binary {
                    left: Box<Expr>,
                    op: BinaryOp,
                    right: Box<Expr>,
                },

                // Unary operations
                Unary {
                    op: UnaryOp,
                    operand: Box<Expr>,
                },

                // Function call
                Call {
                    callee: Box<Expr>,
                    args: Vec<Expr>,
                },

                // Member access
                Member {
                    object: Box<Expr>,
                    field: String,
                },

                // Lambda expression: |params| body
                Lambda {
                    params: Vec<(String, Option<TypeExpr>)>,
                    return_type: Option<TypeExpr>,
                    body: Box<Expr>,
                },

                // If expression
                If {
                    condition: Box<Expr>,
                    then_branch: Box<Expr>,
                    else_branch: Option<Box<Expr>>,
                },

                // Match expression
                Match {
                    scrutinee: Box<Expr>,
                    arms: Vec<MatchArm>,
                },

                // Block expression
                Block {
                    statements: Vec<Statement>,
                    final_expr: Option<Box<Expr>>,
                },

                // Quote: '{ expr }
                Quote(Box<Expr>),

                // Eval: !expr
                Eval(Box<Expr>),

                // Reflect: ?Type
                Reflect(Box<TypeExpr>),
            }

            #[derive(Debug, Clone)]
            pub struct MatchArm {
                pub pattern: Pattern,
                pub guard: Option<Box<Expr>>,
                pub body: Box<Expr>,
            }

            #[derive(Debug, Clone)]
            pub enum Pattern {
                Wildcard,
                Identifier(String),
                Literal(Literal),
                Constructor {
                    name: String,
                    fields: Vec<Pattern>,
                },
                Tuple(Vec<Pattern>),
            }

            /// Statement nodes
            #[derive(Debug, Clone)]
            pub enum Statement {
                // Existing...

                // Variable declaration: name: Type = expr
                Let {
                    name: String,
                    type_ann: Option<TypeExpr>,
                    value: Expr,
                },

                // Assignment: name = expr
                Assign {
                    target: Expr,
                    value: Expr,
                },

                // For loop: for item in iterable { body }
                For {
                    binding: String,
                    iterable: Expr,
                    body: Vec<Statement>,
                },

                // While loop: while condition { body }
                While {
                    condition: Expr,
                    body: Vec<Statement>,
                },

                // Infinite loop: loop { body }
                Loop {
                    body: Vec<Statement>,
                },

                // Break
                Break,

                // Continue
                Continue,

                // Return
                Return(Option<Expr>),

                // Expression statement
                Expr(Expr),
            }
            ```

            ## Part 3: Control Flow Parsing

            Add parsing functions:

            ```rust
            fn parse_if_expr(&mut self) -> Result<Expr, ParseError> {
                self.expect(Token::If)?;
                let condition = self.parse_expr(0)?;
                self.expect(Token::LBrace)?;
                let then_branch = self.parse_block_expr()?;

                let else_branch = if self.check(Token::Else) {
                    self.advance();
                    if self.check(Token::If) {
                        Some(Box::new(self.parse_if_expr()?))
                    } else {
                        self.expect(Token::LBrace)?;
                        Some(Box::new(self.parse_block_expr()?))
                    }
                } else {
                    None
                };

                Ok(Expr::If {
                    condition: Box::new(condition),
                    then_branch: Box::new(then_branch),
                    else_branch,
                })
            }

            fn parse_match_expr(&mut self) -> Result<Expr, ParseError> {
                self.expect(Token::Match)?;
                let scrutinee = self.parse_expr(0)?;
                self.expect(Token::LBrace)?;

                let mut arms = Vec::new();
                while !self.check(Token::RBrace) {
                    let pattern = self.parse_pattern()?;
                    let guard = if self.check(Token::Where) {
                        self.advance();
                        Some(Box::new(self.parse_expr(0)?))
                    } else {
                        None
                    };
                    self.expect(Token::LBrace)?;
                    let body = self.parse_block_expr()?;
                    arms.push(MatchArm { pattern, guard, body: Box::new(body) });
                }

                self.expect(Token::RBrace)?;
                Ok(Expr::Match {
                    scrutinee: Box::new(scrutinee),
                    arms,
                })
            }

            fn parse_for_stmt(&mut self) -> Result<Statement, ParseError> {
                self.expect(Token::For)?;
                let binding = self.expect_identifier()?;
                self.expect(Token::In)?;
                let iterable = self.parse_expr(0)?;
                self.expect(Token::LBrace)?;
                let body = self.parse_statement_list()?;
                self.expect(Token::RBrace)?;

                Ok(Statement::For { binding, iterable, body })
            }

            fn parse_lambda(&mut self) -> Result<Expr, ParseError> {
                self.expect(Token::Bar)?;

                let mut params = Vec::new();
                while !self.check(Token::Bar) {
                    let name = self.expect_identifier()?;
                    let type_ann = if self.check(Token::Colon) {
                        self.advance();
                        Some(self.parse_type()?)
                    } else {
                        None
                    };
                    params.push((name, type_ann));

                    if !self.check(Token::Bar) {
                        self.expect(Token::Comma)?;
                    }
                }
                self.expect(Token::Bar)?;

                let return_type = if self.check(Token::Arrow) {
                    self.advance();
                    Some(self.parse_type()?)
                } else {
                    None
                };

                let body = if self.check(Token::LBrace) {
                    self.expect(Token::LBrace)?;
                    self.parse_block_expr()?
                } else {
                    self.parse_expr(0)?
                };

                Ok(Expr::Lambda {
                    params,
                    return_type,
                    body: Box::new(body),
                })
            }
            ```

            ## Validation

            After modification:
            ```bash
            cargo build
            cargo test parser
            ```

            Test with DOL 2.0 syntax:
            ```bash
            echo 'gene Test { function add(a: Int32, b: Int32) -> Int32 { return a + b } }' | ./target/debug/dol-parse -
            ```

    # ─────────────────────────────────────────────────────────────────
    # TEST UPDATES
    # ─────────────────────────────────────────────────────────────────
    dol-tests-impl:
        role: testing
        model: claude-sonnet-4-20250514
        description: Update test suites for DOL 2.0 syntax

        context: |
            Test files:
            - tests/lexer_tests.rs
            - tests/parser_tests.rs
            - tests/integration_tests.rs

            Must test all new tokens and parsing constructs.

        tools:
            - file_read
            - file_write
            - cargo_test

        inputs:
            - tests/lexer_tests.rs
            - tests/parser_tests.rs
            - src/lexer.rs (after dol-lexer-impl)
            - src/parser.rs (after dol-parser-impl)

        outputs:
            - tests/lexer_tests.rs (modified)
            - tests/parser_tests.rs (modified)
            - tests/dol2_tests.rs (new)

        dependencies:
            - dol-lexer-impl
            - dol-parser-impl

        task: |
            Create comprehensive tests for DOL 2.0 syntax.

            ## Lexer Tests (tests/lexer_tests.rs)

            Add tests for new tokens:

            ```rust
            #[test]
            fn lex_composition_operators() {
                let source = "a |> b >> c @ d := e";
                let tokens = tokenize(source).unwrap();

                assert!(tokens.iter().any(|t| t.token == Token::Pipe));
                assert!(tokens.iter().any(|t| t.token == Token::Compose));
                assert!(tokens.iter().any(|t| t.token == Token::Apply));
                assert!(tokens.iter().any(|t| t.token == Token::Bind));
            }

            #[test]
            fn lex_meta_operators() {
                let source = "'{ x } !expr #macro ?Type";
                let tokens = tokenize(source).unwrap();

                assert!(tokens.iter().any(|t| t.token == Token::Quote));
                assert!(tokens.iter().any(|t| t.token == Token::Bang));
                assert!(tokens.iter().any(|t| t.token == Token::Macro));
                assert!(tokens.iter().any(|t| t.token == Token::Reflect));
            }

            #[test]
            fn lex_control_keywords() {
                let source = "if else match for while loop break continue return in where";
                let tokens = tokenize(source).unwrap();

                assert!(tokens.iter().any(|t| t.token == Token::If));
                assert!(tokens.iter().any(|t| t.token == Token::Else));
                assert!(tokens.iter().any(|t| t.token == Token::Match));
                assert!(tokens.iter().any(|t| t.token == Token::For));
                assert!(tokens.iter().any(|t| t.token == Token::While));
                assert!(tokens.iter().any(|t| t.token == Token::Loop));
                assert!(tokens.iter().any(|t| t.token == Token::Break));
                assert!(tokens.iter().any(|t| t.token == Token::Continue));
                assert!(tokens.iter().any(|t| t.token == Token::Return));
                assert!(tokens.iter().any(|t| t.token == Token::In));
                assert!(tokens.iter().any(|t| t.token == Token::Where));
            }

            #[test]
            fn lex_lambda_syntax() {
                let source = "|x: Int32, y: Int32| -> Int32 { x + y }";
                let tokens = tokenize(source).unwrap();

                assert!(tokens.iter().any(|t| t.token == Token::Bar));
                assert!(tokens.iter().any(|t| t.token == Token::Arrow));
            }

            #[test]
            fn lex_type_keywords() {
                let source = "Int32 Int64 Float64 Bool String Void";
                let tokens = tokenize(source).unwrap();

                assert!(tokens.iter().any(|t| t.token == Token::Int32));
                assert!(tokens.iter().any(|t| t.token == Token::Int64));
                assert!(tokens.iter().any(|t| t.token == Token::Float64));
                assert!(tokens.iter().any(|t| t.token == Token::BoolType));
                assert!(tokens.iter().any(|t| t.token == Token::StringType));
                assert!(tokens.iter().any(|t| t.token == Token::VoidType));
            }

            #[test]
            fn lex_idiom_brackets() {
                let source = "[| f a b |]";
                let tokens = tokenize(source).unwrap();

                assert!(tokens.iter().any(|t| t.token == Token::IdiomOpen));
                assert!(tokens.iter().any(|t| t.token == Token::IdiomClose));
            }
            ```

            ## Parser Tests (tests/parser_tests.rs)

            Add tests for new constructs:

            ```rust
            #[test]
            fn parse_pipe_expression() {
                let source = r#"
                    gene Test {
                        function process() -> Int32 {
                            return data |> transform |> validate
                        }
                    }
                "#;
                let result = parse(source);
                assert!(result.is_ok());
            }

            #[test]
            fn parse_compose_expression() {
                let source = r#"
                    gene Test {
                        function pipeline() -> Function<Int32, Int32> {
                            return double >> increment >> square
                        }
                    }
                "#;
                let result = parse(source);
                assert!(result.is_ok());
            }

            #[test]
            fn parse_if_expression() {
                let source = r#"
                    gene Test {
                        function classify(x: Int32) -> String {
                            if x > 0 {
                                return "positive"
                            } else if x < 0 {
                                return "negative"
                            } else {
                                return "zero"
                            }
                        }
                    }
                "#;
                let result = parse(source);
                assert!(result.is_ok());
            }

            #[test]
            fn parse_match_expression() {
                let source = r#"
                    gene Test {
                        function describe(opt: Optional<Int32>) -> String {
                            match opt {
                                Some(n) where n > 0 {
                                    return "positive"
                                }
                                Some(n) {
                                    return "non-positive"
                                }
                                None {
                                    return "nothing"
                                }
                            }
                        }
                    }
                "#;
                let result = parse(source);
                assert!(result.is_ok());
            }

            #[test]
            fn parse_for_loop() {
                let source = r#"
                    gene Test {
                        function sum(items: Slice<Int32>) -> Int32 {
                            total: Int32 = 0
                            for item in items {
                                total = total + item
                            }
                            return total
                        }
                    }
                "#;
                let result = parse(source);
                assert!(result.is_ok());
            }

            #[test]
            fn parse_while_loop() {
                let source = r#"
                    gene Test {
                        function countdown(n: Int32) -> Void {
                            while n > 0 {
                                print(n)
                                n = n - 1
                            }
                        }
                    }
                "#;
                let result = parse(source);
                assert!(result.is_ok());
            }

            #[test]
            fn parse_lambda() {
                let source = r#"
                    gene Test {
                        function apply_twice(f: Function<Int32, Int32>, x: Int32) -> Int32 {
                            doubled = items |> map(|n: Int32| -> Int32 { n * 2 })
                            return f(f(x))
                        }
                    }
                "#;
                let result = parse(source);
                assert!(result.is_ok());
            }

            #[test]
            fn parse_operator_precedence() {
                // Test: a |> f >> g @ x := h
                // Should parse as: (a |> ((f >> g) @ x)) := h
                let source = r#"
                    gene Test {
                        function chain() -> Result<Int32, Error> {
                            result = a |> f >> g @ x := h
                            return result
                        }
                    }
                "#;
                let result = parse(source);
                assert!(result.is_ok());
                // TODO: Verify AST structure matches expected precedence
            }

            #[test]
            fn parse_quote_eval() {
                let source = r#"
                    gene Test {
                        function meta() -> Int32 {
                            ast = '{ x + y * 2 }
                            return !ast
                        }
                    }
                "#;
                let result = parse(source);
                assert!(result.is_ok());
            }
            ```

            ## Integration Tests (tests/dol2_tests.rs - new file)

            ```rust
            //! DOL 2.0 Integration Tests
            //! End-to-end tests for new syntax

            use metadol::{parse, validate};

            #[test]
            fn full_dol2_gene() {
                let source = r#"
                    module test.dol2 @ 1.0.0

                    gene Calculator {
                        type: {
                            value: Int32
                        }

                        function add(self, n: Int32) -> Int32 {
                            return self.value + n
                        }

                        function transform(self, f: Function<Int32, Int32>) -> Calculator {
                            return Calculator { value: f(self.value) }
                        }

                        function pipeline(self) -> Int32 {
                            return self.value
                                |> double
                                |> increment
                                |> square
                        }

                        exegesis {
                            Calculator demonstrates DOL 2.0 functional syntax.
                        }
                    }
                "#;

                let parsed = parse(source).expect("Should parse");
                let validated = validate(&parsed).expect("Should validate");
            }

            #[test]
            fn backward_compatibility() {
                // All DOL 1.x files should still parse
                let dol1_files = [
                    "examples/genes/container.exists.dol",
                    "examples/traits/container.lifecycle.dol",
                    "examples/systems/univrs.orchestrator.dol",
                ];

                for file in &dol1_files {
                    let source = std::fs::read_to_string(file)
                        .expect(&format!("Should read {}", file));
                    let result = parse(&source);
                    assert!(result.is_ok(), "DOL 1.x file {} should parse", file);
                }
            }
            ```

            ## Validation

            Run all tests:
            ```bash
            cargo test
            cargo test --release
            ```

            Verify backward compatibility:
            ```bash
            ./target/debug/dol-check examples/
            ./target/debug/dol-check stdlib/
            ```

# ═══════════════════════════════════════════════════════════════════
# WORKFLOWS
# ═══════════════════════════════════════════════════════════════════

workflows:
    impl-phase-1:
        description: "DOL 2.0 Lexer + Parser + Tests"
        stages:
            - name: lexer
              agents: [dol-lexer-impl]

            - name: parser
              agents: [dol-parser-impl]
              depends_on: [lexer]

            - name: tests
              agents: [dol-tests-impl]
              depends_on: [parser]

            - name: validation
              run: |
                  cargo test --release
                  ./target/release/dol-check examples/
                  ./target/release/dol-check stdlib/
                  echo "✓ Phase 1 complete: DOL 2.0 lexer + parser implemented"

    quick-test:
        description: "Run tests only"
        stages:
            - name: test
              run: cargo test

# ═══════════════════════════════════════════════════════════════════
# CHECKPOINTS
# ═══════════════════════════════════════════════════════════════════

checkpoints:
    lexer-complete:
        trigger: command_success
        command: "cargo test lexer"
        action: notify
        message: "✓ Lexer extended with DOL 2.0 tokens"

    parser-complete:
        trigger: command_success
        command: "cargo test parser"
        action: notify
        message: "✓ Parser extended with DOL 2.0 syntax"

    backward-compat:
        trigger: command_success
        command: "./target/release/dol-check examples/ && ./target/release/dol-check stdlib/"
        action: notify
        message: "✓ Backward compatibility verified: 130/130 files pass"

    phase1-complete:
        trigger: all_checkpoints
        checkpoints: [lexer-complete, parser-complete, backward-compat]
        action: celebrate
        message: |
            ═══════════════════════════════════════════════════
            ✓ DOL 2.0 PHASE 1 COMPLETE

            Lexer: Composition, meta, control tokens added
            Parser: Pratt parser, control flow, lambdas
            Tests: Full coverage, backward compatible

            Next: Type Checker (dol-typecheck-impl)
            ═══════════════════════════════════════════════════
